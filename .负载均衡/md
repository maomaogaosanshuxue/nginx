负载均衡
单个服务器解决不了，我们增加服务器的数量，然后将请求分发到各个服务器上,将原先
请求集中到单个服务器上的情况改为将请求分发到多个服务器上,将负载分发到不同的服
务器，也就是我们所说的负载均衡
tomcat:  47.75.174.233:8080  webapps 创建edu文件 
tomcat   47.107.105.174:8080  webapps 创建edu文件
# 负载均衡
#gzip on；
upstream myserver { #**********
        server 47.75.174.233:8080; 
        server 47.107.105.174:8080;
server {
        listen       80;
        server_name  47.113.122.218;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            root   html;
            proxy_pass http://myserver; #*****          
            index  index.html index.htm;
        }
      
 负载分配策略
在linux下有Nginx、LVS、 Haproxy 等等服务可以提供负载均衡服务，而且Nginx提供了几种分配方式(策略):。
1、轮询(默认)

每个请求按时间顺序逐一分配到不 同的后端服务器，如果后端服务器down掉，能自动剔除

2、weight
weight代表权重默认为1,权重越高被分配的客户端越多。
指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。例如: 。
upstream myserver {
        server 47.75.174.233:8080   weight=10; 
        server 47.107.105.174:8080   weight=1;


3、ip hash
每个请求按访问ip的hash结果分配, 这样每个访客固定访问一个后端服务器,可以解诀session的问题

upstream myserver{
  ip_ hash
  server 47.75.174.233:8080
  server 47.107.105.174:8080 
}

fair (第三方) 按后端服务器的响应时间来分配请求，响应时间短的优先分配
upstream myserver{
  server 47.75.174.233:8080
  server 47.107.105.174:8080 
  fair
